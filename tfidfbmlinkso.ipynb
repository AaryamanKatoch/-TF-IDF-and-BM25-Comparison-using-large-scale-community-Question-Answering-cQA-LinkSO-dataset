{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLDMbffhWsW8"
      },
      "source": [
        "# Overview \n",
        "\n",
        "This project is focused on understanding TF-IDF and BM25 algorithms through implementing them from scratch, and applying them on the large-scale community Question-Answering (cQA) [LinkSO](https://dl.acm.org/doi/10.1145/3283812.3283815) dataset. \n",
        "\n",
        "pipeline:\n",
        "\n",
        "- Step 0: load the dataset into the current environment;\n",
        "- Step 1: import the libraries and helper functions;\n",
        "- Step 2: preprocess the dataset to get the required inputs for the TF-IDF and BM25 algorithms;\n",
        "- Step 3: implement the TF-IDF and BM25 algorithms.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgvp42c9ehAL"
      },
      "source": [
        "## Step 0 - Load Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_KOkCPIiP-_"
      },
      "source": [
        "## Step 1 - Import Libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uiXyxfoW0JD"
      },
      "source": [
        "Importing the required libraries here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5GqfSBrBWgJg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "import pathlib\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from cs589.utils.common import save_pickle_file, load_pickle_file, load_text_file\n",
        "\n",
        "base_path = pathlib.Path(\"cs589/dataset/\")\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dIey_8x0iTM0"
      },
      "outputs": [],
      "source": [
        "def split_text(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def load_qids(lang=\"java\"):\n",
        "    return [qid.strip(string.whitespace) for qid in load_text_file(base_path / pathlib.Path(f\"{lang}/{lang}_test_qid.txt\"))]\n",
        "\n",
        "\n",
        "def load_qid_dataframe(lang=\"java\"):\n",
        "    qid_dataframe = pd.read_csv(base_path / pathlib.Path(f\"{lang}/{lang}_cosidf.txt\"), \n",
        "                                sep=\"\\t\", \n",
        "                                usecols=[\"qid1\", \"qid2\", \"label\"],\n",
        "                                dtype={\"qid1\": str, \"qid2\": str, \"label\": int})\n",
        "    return qid_dataframe\n",
        "\n",
        "\n",
        "def load_corpus(lang=\"java\", verbose=False):\n",
        "    lines = load_text_file(base_path / pathlib.Path(f\"{lang}/{lang}_qid2all.txt\"))\n",
        "\n",
        "    record_list = list()\n",
        "    for line in tqdm(lines, disable=not verbose):\n",
        "        record_list.append(\n",
        "            {name: text.strip(string.whitespace) for name, text in zip([\"qid\", \"title\", \"question\", \"answer\"], line.split(\"\\t\"))}\n",
        "        )\n",
        "            \n",
        "    corpus_dataframe = pd.DataFrame(record_list)\n",
        "\n",
        "    return corpus_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4rylui-wcM6",
        "outputId": "bd8b06ea-a876-4cc0-e6b0-291b42c5804a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 159263/159263 [00:00<00:00, 181351.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        qid                                     title  \\\n",
            "0  31424546   eclipse mars starts exit code using jdk   \n",
            "1  31457289  efficient method updating observablelist   \n",
            "2  16777228                          set title jtable   \n",
            "3  27262998                  multiple websockets java   \n",
            "4  46137348      find runtime error nzec java program   \n",
            "\n",
            "                                            question  \\\n",
            "0  plan moving eclipse mars recently installed bi...   \n",
            "1  setup mysql database data makeshift server bui...   \n",
            "2  newbie java wanted set table header jtable tak...   \n",
            "3  deprecated ok opening connection specific port...   \n",
            "4  find runtime error nzec java program program r...   \n",
            "\n",
            "                                              answer  \n",
            "0  jdk bit download windows x version point vm mi...  \n",
            "1  need would work keep list instance serverlist ...  \n",
            "2  define variable containing column names must i...  \n",
            "3  trying achieve multiple function listen server...  \n",
            "4  try test code input like probably shall realiz...  \n"
          ]
        }
      ],
      "source": [
        "# take a look at the corpus\n",
        "pd.set_option(\"display.max_columns\", 10)\n",
        "\n",
        "java_corpus_dataframe = load_corpus(lang=\"java\", verbose=True)\n",
        "print(java_corpus_dataframe.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruBPtuUEjMpY"
      },
      "source": [
        "## Step 2 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jalvAG2AnNlE"
      },
      "source": [
        "\n",
        "\n",
        "The following cell computes the term frequency (TF) for each word in each component in each StackOverflow question (indexed by the question ID `qid`). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1A8rjYW5PZUT"
      },
      "outputs": [],
      "source": [
        "def get_corpus_tf_dict(corpus_dataframe):\n",
        "    \"\"\" Input: corpus_dataframe, e.g.,    \n",
        "    \n",
        "         qid         title                 question          answer\n",
        " 0  31424546   eclipse mars   eclipse moving eclipse    jdk download   \n",
        "                                            \n",
        "        Output: corpus_tf_dict, the term frequency for each word in each component of each question, e.g., \n",
        "        {'31424546': {'title': {'eclipse': 1, 'mars': 1},\n",
        "                      'question': {'moving': 1, 'eclipse': 2},\n",
        "                      'answer': {'jdk': 1, 'download': 1}}}\n",
        "    \"\"\"\n",
        "    cnt_dataframe = copy.deepcopy(corpus_dataframe)\n",
        "    for c in [\"title\", \"question\", \"answer\"]:\n",
        "        cnt_dataframe[c] = cnt_dataframe[c].progress_apply(lambda x: Counter(split_text(x)))\n",
        "\n",
        "    corpus_tf_dict = cnt_dataframe.set_index(\"qid\").to_dict(\"index\")\n",
        "   \n",
        "    return corpus_tf_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNmHc7CWPXC9"
      },
      "source": [
        "\n",
        "\n",
        "The following cell computes the document length (dl) of each component in each StackOverflow question (indexed by the question ID `qid`). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7BrSk87sPTvm"
      },
      "outputs": [],
      "source": [
        "def get_corpus_dl_dict(corpus_dataframe):\n",
        "    \"\"\" Input: corpus_dataframe, e.g.,    \n",
        "         qid         title                 question          answer\n",
        "0  31424546   eclipse mars   eclipse moving eclipse    jdk download  \n",
        "\n",
        "        Output: corpus_dl_dict, the document length for each component from each question, e.g., \n",
        "        {'31424546': {'title': 2,\n",
        "                      'question': 3,\n",
        "                      'answer': 2}}\n",
        "    \"\"\"\n",
        "    length_dataframe = copy.deepcopy(corpus_dataframe)\n",
        "    for c in [\"title\", \"question\", \"answer\"]:\n",
        "        length_dataframe[c] = length_dataframe[c].progress_apply(lambda x: len(split_text(x)))\n",
        "\n",
        "    corpus_dl_dict = length_dataframe.set_index(\"qid\").to_dict(\"index\")\n",
        "    \n",
        "    return corpus_dl_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDOb8YP_PQ4W"
      },
      "source": [
        "The following cell computes the document frequency (DF) of each word in each StackOverflow question (indexed by the question ID `qid`). The definition of document frequency is how many document a word appears in, not to be confused with the word's frequency in the entire corpus. For example, the df of \"eclipse\" below is 2 instead of 3. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A-9_U7q9PKW-"
      },
      "outputs": [],
      "source": [
        "def get_corpus_df_dict(corpus_dataframe):\n",
        "    \"\"\" Input: corpus_dataframe, e.g.,    \n",
        "         qid          title                 question          answer\n",
        " 0  31424546   eclipse mars   eclipse moving eclipse    jdk download  \n",
        "\n",
        "        Output: corpus_df_dict, the document length for each component from each question, e.g., \n",
        "        {'eclipse': 2, \"mars\": 1, \"moving\": 1, \"jdk\": 1, \"download\": 1}\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "    X = vectorizer.fit_transform(corpus_dataframe.title.tolist() + \\\n",
        "                                 corpus_dataframe.question.tolist() + \\\n",
        "                                 corpus_dataframe.answer.tolist())\n",
        "    corpus_df_dict = {token: doc_freq for token, doc_freq in \\\n",
        "                      zip(vectorizer.get_feature_names(), np.ravel(X.sum(axis=0)))}\n",
        " \n",
        "    return corpus_df_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky9DEE-3PHQ2"
      },
      "source": [
        "### Saving the Data Preprocessing Result\n",
        "\n",
        "After computing the TF, DF and dl, cache each of them in a pickle file to be loaded later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VHnTVSnDO8a4"
      },
      "outputs": [],
      "source": [
        "pkl_path = pathlib.Path(\"pkl/\")\n",
        "if not pkl_path.exists(): pkl_path.mkdir()\n",
        "\n",
        "def save_preprocessing_results(lang):\n",
        "    print(f\"Processing {lang}...\")\n",
        "        \n",
        "    lang_pkl_path = pkl_path / lang\n",
        "    if not lang_pkl_path.exists(): os.mkdir(lang_pkl_path)\n",
        "\n",
        "    # load corpus and convert corpus to various required data\n",
        "    corpus_dataframe = load_corpus(lang=lang, verbose=True)\n",
        "\n",
        "    # obtain the dictionary for the term frequency for each word in each component of each question\n",
        "    corpus_tf_dict = get_corpus_tf_dict(corpus_dataframe)\n",
        "\n",
        "    # saving the term frequency dictionary\n",
        "    save_pickle_file(corpus_tf_dict, f\"pkl/{lang}/corpus_tf_dict.pkl\")\n",
        "\n",
        "    # obtain the dictionary for the document length for each component in each question \n",
        "    corpus_dl_dict = get_corpus_dl_dict(corpus_dataframe)\n",
        "    \n",
        "    # save the document length dictionary\n",
        "    save_pickle_file(corpus_dl_dict, f\"pkl/{lang}/corpus_dl_dict.pkl\")\n",
        "\n",
        "    # obtain the dictionary for the document frequency for each word in the corpus\n",
        "    corpus_df_dict = get_corpus_df_dict(corpus_dataframe)\n",
        "\n",
        "    # remove rare words\n",
        "    corpus_df_dict = {k: v for k, v in corpus_df_dict.items() if v >= 20}\n",
        "\n",
        "    # save the document frequency dictionary\n",
        "    save_pickle_file(corpus_df_dict, f\"pkl/{lang}/corpus_df_dict.pkl\")\n",
        "\n",
        "    return corpus_tf_dict, corpus_dl_dict, corpus_df_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25NkbjmfLCVN"
      },
      "source": [
        "Run the data processing pipeline for the 3 languages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plzD8Kdn84oI",
        "outputId": "5495e8c1-e171-4f99-c822-3b73e823d55e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing python...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 128500/128500 [00:00<00:00, 247652.70it/s]\n",
            "100%|██████████| 128500/128500 [00:00<00:00, 155410.39it/s]\n",
            "100%|██████████| 128500/128500 [00:02<00:00, 48251.82it/s]\n",
            "100%|██████████| 128500/128500 [00:02<00:00, 44611.44it/s]\n",
            "100%|██████████| 128500/128500 [00:00<00:00, 537171.23it/s]\n",
            "100%|██████████| 128500/128500 [00:00<00:00, 241356.69it/s]\n",
            "100%|██████████| 128500/128500 [00:00<00:00, 226468.21it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing java...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 159263/159263 [00:00<00:00, 301252.29it/s]\n",
            "100%|██████████| 159263/159263 [00:00<00:00, 172133.10it/s]\n",
            "100%|██████████| 159263/159263 [00:02<00:00, 61518.93it/s]\n",
            "100%|██████████| 159263/159263 [00:04<00:00, 39378.86it/s]\n",
            "100%|██████████| 159263/159263 [00:00<00:00, 510243.04it/s]\n",
            "100%|██████████| 159263/159263 [00:00<00:00, 240018.86it/s]\n",
            "100%|██████████| 159263/159263 [00:00<00:00, 206062.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing javascript...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 174015/174015 [00:00<00:00, 312073.47it/s]\n",
            "100%|██████████| 174015/174015 [00:00<00:00, 194838.86it/s]\n",
            "100%|██████████| 174015/174015 [00:02<00:00, 62761.05it/s]\n",
            "100%|██████████| 174015/174015 [00:03<00:00, 52229.72it/s]\n",
            "100%|██████████| 174015/174015 [00:00<00:00, 544158.63it/s]\n",
            "100%|██████████| 174015/174015 [00:00<00:00, 234448.99it/s]\n",
            "100%|██████████| 174015/174015 [00:00<00:00, 206460.17it/s]\n"
          ]
        }
      ],
      "source": [
        "for lang in [\"python\", \"java\", \"javascript\"]:\n",
        "     save_preprocessing_results(lang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_VnIkthWLjos"
      },
      "outputs": [],
      "source": [
        "result_path = pathlib.Path(\"result\")\n",
        "if not result_path.exists(): result_path.mkdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mmb3lSFrdNo"
      },
      "source": [
        "## Step 3 - Implement the TF-IDF and BM25 Algorithms\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MEfOaOj_xqNU"
      },
      "outputs": [],
      "source": [
        "def compute_cosine_similarity(query_tf_dict, \n",
        "                              candidate_tf_dict):\n",
        "    \"\"\" Input: query_tf_dict: a dict of word and its term frequency in query document, e.g.\n",
        "               {\"i\": 1, \"love\": 1, \"python\": 1}\n",
        "               candidate_tf_dict: a dict of word and its term frequency in the candidate document, e.g.\n",
        "               {\"i\": 1, \"like\": 1, \"c++\": 1}\n",
        "        Output: score: cosine similary between query and candidate documents\n",
        "                0.33333333333333337\n",
        "                \n",
        "    \"\"\"\n",
        "\n",
        "    score = 0\n",
        "\n",
        "    commonkeys = list(query_tf_dict.keys() & candidate_tf_dict.keys())\n",
        "    normalquery= math.sqrt(sum(x*x for x in query_tf_dict.values()))\n",
        "    normalcandidate=math.sqrt(sum(y*y for y in candidate_tf_dict.values()))\n",
        "    listquery = list(query_tf_dict[k] for k in commonkeys)\n",
        "    listcandidate = list(candidate_tf_dict[l] for l in commonkeys)\n",
        "  \n",
        "  \n",
        "    cosinesimilarity = np.dot(listquery,listcandidate) / (normalquery * normalcandidate)\n",
        "    score= cosinesimilarity\n",
        "    return score\n",
        "    ##############################################END HERE##############################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-drAVVQqWOjU"
      },
      "source": [
        "Test  `compute_cosine_similarity` implementation on the Python corpus when retrieving candidate's title using query's title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlvkvFCNWMq4",
        "outputId": "4115c5f9-b3e2-46cc-f602-f55aa9327615"
      },
      "outputs": [],
      "source": [
        "lang = \"python\"\n",
        "\n",
        "corpus_tf_dict = load_pickle_file(f\"pkl/{lang}/corpus_tf_dict.pkl\")\n",
        "qid_dataframe = load_qid_dataframe(f\"{lang}\")\n",
        "\n",
        "result_dict = dict()\n",
        "for qid1, qid2 in list(qid_dataframe[[\"qid1\", \"qid2\"]].to_records(index=False)):\n",
        "    result_dict[(qid1, qid2)] = compute_cosine_similarity(corpus_tf_dict[qid1][\"title\"],\n",
        "                                                          corpus_tf_dict[qid2][\"title\"])\n",
        "\n",
        "\n",
        "result_filename = pathlib.Path(\"result/Q4.txt\")\n",
        "if result_filename.exists(): os.remove(result_filename)\n",
        "\n",
        "with open(result_filename, \"a\") as fp:\n",
        "    fp.write(\"qid1\\tqid2\\tscore\\n\")\n",
        "    for (qid1, qid2), score in result_dict.items():\n",
        "        fp.write(f\"{qid1}\\t{qid2}\\t{score}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mekQGz4_xs-N"
      },
      "outputs": [],
      "source": [
        "def compute_document_tfidf(document_tf_dict, \n",
        "                           corpus_df_dict):\n",
        "    \"\"\" Input: document_tf_dict: a dict of word and its term frequency in document\n",
        "               {\"i\": 1, \"love\": 1, \"python\": 1}\n",
        "               corpus_df_dict: a dict of word and its document frequencey in the entire corpus\n",
        "               {\"i\": 2, \"you\": 1, \"we\": 3, \"love\": 1, \"like\": 1, \"hate\": 2, \"python\": 5, \"c++\": 3}\n",
        "        Output: document_word_tfidf_dict: a dict of word and its TF-IDF score in the document\n",
        "               {'i': 13.592366256649782, 'love': 14.103192380416024, 'python': 12.803907396283263}\n",
        "    \"\"\"\n",
        "\n",
        "    document_word_tfidf_dict = dict()\n",
        "\n",
        "    tfDict = {}\n",
        "    counter = sum(document_tf_dict.values())\n",
        "    \n",
        "    for word, count in document_tf_dict.items():\n",
        "      tfDict[word] = count/float(counter)\n",
        "    \n",
        "    \n",
        "    idfDict = {}\n",
        "    N = 10**6;\n",
        "   \n",
        "    #idfDict = dict.fromkeys(document_tf_dict.keys(), 0)\n",
        "    for word, val in corpus_df_dict.items():\n",
        "      if word not in corpus_df_dict.keys(): continue\n",
        "      idfDict[word] = np.log2(N / (float(val+1)))\n",
        "        \n",
        "    \n",
        "    for word, val in tfDict.items():\n",
        "        if word not in idfDict.keys(): continue\n",
        "        document_word_tfidf_dict[word] = val*idfDict[word]\n",
        "    return document_word_tfidf_dict\n",
        "    ##############################################END HERE##############################################\n",
        "    \n",
        "#print(compute_document_tfidf({\"i\": 1, \"love\": 1, \"python\": 1},{\"i\": 2, \"you\": 1, \"we\": 3, \"love\": 1, \"like\": 1, \"hate\": 2, \"python\": 5, \"c++\": 3}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxnFWDzQX65s"
      },
      "source": [
        "Test `compute_document_tfidf` implementation on the title component of the Java corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ev6FSna6Wws8"
      },
      "outputs": [],
      "source": [
        "lang = \"java\"\n",
        "\n",
        "corpus_tf_dict = load_pickle_file(f\"pkl/{lang}/corpus_tf_dict.pkl\")\n",
        "corpus_df_dict = load_pickle_file(f\"pkl/{lang}/corpus_df_dict.pkl\")\n",
        "qid_dataframe = load_qid_dataframe(f\"{lang}\")\n",
        "\n",
        "result_dict = dict()\n",
        "for qid1 in qid_dataframe.qid1.tolist():\n",
        "    result_dict[qid1] = compute_document_tfidf(corpus_tf_dict[qid1][\"title\"],\n",
        "                                               corpus_df_dict)\n",
        "\n",
        "result_filename = pathlib.Path(\"result/Q5.txt\")\n",
        "if result_filename.exists(): os.remove(result_filename)\n",
        "\n",
        "with open(result_filename, \"a\") as fp:\n",
        "    fp.write(\"qid1\\ttoken\\ttfidf\\n\")\n",
        "    for qid1, d in result_dict.items():\n",
        "        for token, score in d.items():\n",
        "            fp.write(f\"{qid1}\\t{token}\\t{score}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m60k2-k4xhRN"
      },
      "source": [
        "\n",
        "\n",
        "Compute the BM25 score between `query_tf_dict` and `candidate_tf_dict`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hfd6rQiGPs-N"
      },
      "outputs": [],
      "source": [
        "def compute_document_bm25(query_tf_dict, \n",
        "                          candidate_tf_dict, \n",
        "                          corpus_df_dict,\n",
        "                          candidate_length,\n",
        "                          avgdl):\n",
        "    \"\"\" Input: query_tf_dict: a dict of word and its term frequency in query document\n",
        "               {\"i\": 1, \"love\": 1, \"python\": 1}     \n",
        "               candidate_tf_dict:a dict of word and its term frequency in candidate document\n",
        "               {\"i\": 1, \"like\": 1, \"c++\": 1}\n",
        "               corpus_df_dict: a dict of word and its document frequencey in the entire corpus\n",
        "               {\"i\": 2, \"you\": 1, \"we\": 3, \"love\": 1, \"like\": 1, \"hate\": 2, \"python\": 5, \"c++\": 3}\n",
        "               candidate_length: number of words in candidate document\n",
        "               3\n",
        "               avgdl: average document length in the entire corpus\n",
        "               4\n",
        "       Output: score: BM25 score between query and candidate\n",
        "               15.816571644101565\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # hyperparameters for BM25 algorithm\n",
        "    k1, b = 3, 0.75\n",
        "    N = 10**6;\n",
        "    score = 0\n",
        "\n",
        "    tf ={};bm25={};idf={}\n",
        "    for term,count in query_tf_dict.items():\n",
        "        if term in candidate_tf_dict.keys():\n",
        "          tf[term] = (candidate_tf_dict[term] * (k1+1))/((candidate_tf_dict[term])+(k1*(1-b+(b*(candidate_length/avgdl)))))\n",
        "        else:\n",
        "          tf[term] = 0\n",
        "  #  idf = dict.fromkeys(candidate_tf_dict.keys(), 0)   \n",
        "    for term,count in query_tf_dict.items():\n",
        "      if (term not in candidate_tf_dict.keys()) or (term not in corpus_df_dict.keys()): continue\n",
        "      idf[term]= np.log(((N-corpus_df_dict[term]+0.5)/(corpus_df_dict[term]+0.5))+1)\n",
        "    \n",
        "    for term,count in tf.items():\n",
        "      if term in idf.keys():\n",
        "        bm25[term]=count * idf[term]\n",
        "    score = sum(bm25.values())\n",
        "    \n",
        "    \n",
        "    ##############################################END HERE##############################################\n",
        "    return score\n",
        "    \n",
        "\n",
        "    \n",
        "    ##############################################END HERE##############################################\n",
        "\n",
        "#print(compute_document_bm25({\"i\": 1, \"love\": 1, \"python\": 1}, \n",
        " #                        {\"i\": 1, \"like\": 1, \"c++\": 1}, \n",
        "  #                        {\"i\": 2, \"you\": 1, \"we\": 3, \"love\": 1, \"like\": 1, \"hate\": 2, \"python\": 5, \"c++\": 3},\n",
        "   #                       3,\n",
        "    #                      4)) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAt0Crk9YF9x"
      },
      "source": [
        "Test  `compute_document_bm25` implementation on the `title` component of the JavaScript corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R3ncHWaX4Ol",
        "outputId": "ee643247-f5e1-4c39-dec2-ae1e81f07310"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 174015/174015 [00:00<00:00, 307715.58it/s]\n"
          ]
        }
      ],
      "source": [
        "lang = \"javascript\"\n",
        "\n",
        "corpus_tf_dict = load_pickle_file(f\"pkl/{lang}/corpus_tf_dict.pkl\")\n",
        "corpus_df_dict = load_pickle_file(f\"pkl/{lang}/corpus_df_dict.pkl\")\n",
        "corpus_dl_dict = load_pickle_file(f\"pkl/{lang}/corpus_dl_dict.pkl\")\n",
        "\n",
        "qid_dataframe = load_qid_dataframe(f\"{lang}\")\n",
        "\n",
        "corpus_dataframe = load_corpus(lang=lang, verbose=True)\n",
        "avgdl = corpus_dataframe[\"title\"].apply(lambda x: len(split_text(x))).sum() / len(corpus_dataframe)\n",
        "\n",
        "result_dict = dict()\n",
        "for qid1, qid2 in list(qid_dataframe[[\"qid1\", \"qid2\"]].to_records(index=False)):\n",
        "    result_dict[(qid1, qid2)] = compute_document_bm25(corpus_tf_dict[qid1][\"title\"],\n",
        "                                                      corpus_tf_dict[qid2][\"title\"],\n",
        "                                                      corpus_df_dict,\n",
        "                                                      corpus_dl_dict[qid2][\"title\"],\n",
        "                                                      avgdl)\n",
        "\n",
        "\n",
        "result_filename = pathlib.Path(\"result/Q6.txt\")\n",
        "if result_filename.exists(): os.remove(result_filename)\n",
        "\n",
        "with open(result_filename, \"a\") as fp:\n",
        "    fp.write(\"qid1\\tqid2\\tscore\\n\")\n",
        "    for (qid1, qid2), score in result_dict.items():\n",
        "        fp.write(f\"{qid1}\\t{qid2}\\t{score}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adiHX2tn2IXu"
      },
      "source": [
        "### Running Ranking Algorithms\n",
        "\n",
        "The function `run_retrieval_algorithm` puts implementations (`compute_cosine_similarity`, `compute_document_tfidf`, and `compute_document_bm25`) together and apply them to the entire dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uNkmuEEjmDC7"
      },
      "outputs": [],
      "source": [
        "base_path = pathlib.Path(\"cs589/dataset/\")\n",
        "\n",
        "def run_retrieval_algorithm(lang, algo, component, qid1s=None):\n",
        "    corpus_tf_dict = load_pickle_file(f\"pkl/{lang}/corpus_tf_dict.pkl\")\n",
        "    corpus_dl_dict = load_pickle_file(f\"pkl/{lang}/corpus_dl_dict.pkl\")\n",
        "    corpus_df_dict = load_pickle_file(f\"pkl/{lang}/corpus_df_dict.pkl\")\n",
        "\n",
        "    corpus_dataframe = load_corpus(lang=lang, verbose=False)\n",
        "    available_ids = corpus_dataframe.qid.unique()\n",
        "    avgdl = corpus_dataframe[component].apply(lambda x: len(split_text(x))).sum() / len(corpus_dataframe)\n",
        "\n",
        "    qid1s = qid1s if qid1s != None else load_qids(lang=lang)\n",
        "    qid1_dataframe = load_qid_dataframe(lang=lang)\n",
        "    \n",
        "    result_folder = pathlib.Path(\"result/\")\n",
        "    if not result_folder.exists(): result_folder.mkdir()\n",
        "\n",
        "    result_filename = pathlib.Path(f\"result/{lang}_{algo}_{component}.txt\")\n",
        "\n",
        "    # remove existing result file\n",
        "    if result_filename.exists():\n",
        "        os.remove(result_filename)\n",
        "\n",
        "    # write header\n",
        "    with open(result_filename, \"a\") as fp:\n",
        "        fp.write(\"qid1\\tqid2\\tscore\\tlabel\\n\")\n",
        "    \n",
        "    for qid1 in tqdm(qid1s):\n",
        "        if qid1 not in available_ids: continue\n",
        "\n",
        "        cond1 = qid1_dataframe.qid1 == qid1\n",
        "        cond2 = qid1_dataframe.label == 1\n",
        "\n",
        "        qid2s = qid1_dataframe[cond1].qid2.tolist()\n",
        "        qid2s_linked = qid1_dataframe[cond1 & cond2].qid2.tolist()\n",
        "\n",
        "        qid1_tf_dict = corpus_tf_dict[qid1][\"title\"]\n",
        "        query_result = dict()\n",
        "\n",
        "        # only for BM25\n",
        "        max_bm25 = -1\n",
        "        for qid2 in qid2s:\n",
        "            if qid2 not in available_ids: continue\n",
        "\n",
        "            qid2_tf_dict = corpus_tf_dict[qid2][component]\n",
        "\n",
        "            # tfidf\n",
        "            if algo == \"tfidf\":\n",
        "                score = compute_cosine_similarity(compute_document_tfidf(qid1_tf_dict, corpus_df_dict),\n",
        "                                                  compute_document_tfidf(qid2_tf_dict, corpus_df_dict))\n",
        "            \n",
        "            # bm25\n",
        "            if algo == \"bm25\":\n",
        "                candidate_length = corpus_dl_dict[qid2][component]\n",
        "                score = compute_document_bm25(qid1_tf_dict, \n",
        "                                              qid2_tf_dict, \n",
        "                                              corpus_df_dict,\n",
        "                                              candidate_length,\n",
        "                                              avgdl)\n",
        "                \n",
        "                max_bm25 = max(score, max_bm25)\n",
        "            \n",
        "            query_result[qid2] = score\n",
        "        \n",
        "        # adjust BM25 score\n",
        "        if (algo == \"bm25\") and (max_bm25 != 0):\n",
        "            query_result = {qid: score / max_bm25 for qid, score in query_result.items()}\n",
        "        \n",
        "        qid2s_sorted = sorted(query_result, key=query_result.get, reverse=True)\n",
        "\n",
        "        with open(result_filename, \"a\") as fp:\n",
        "            for qid2 in qid2s_sorted:\n",
        "                label = 1 if qid2 in qid2s_linked else 0\n",
        "                score = query_result[qid2]\n",
        "                \n",
        "                fp.write(f\"{qid1}\\t{qid2}\\t{score}\\t{label}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n44keLmELb4D"
      },
      "source": [
        "Run the retrieval algorithms and save the ranking results for each language and each retrieval algorithms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWTUeO5lry_N",
        "outputId": "391b7665-3e60-42fa-ad93-f5ed3817ab85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running bm25 on python's title...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [06:49<00:00,  2.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running bm25 on python's question...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [06:42<00:00,  2.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running bm25 on python's answer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [06:25<00:00,  2.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running tfidf on python's title...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 715/1000 [20:38<08:37,  1.82s/it]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in true_divide\n",
            "100%|██████████| 1000/1000 [29:07<00:00,  1.75s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running tfidf on python's question...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [30:32<00:00,  1.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running tfidf on python's answer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [30:21<00:00,  1.82s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running bm25 on java's title...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [09:43<00:00,  1.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running bm25 on java's question...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [09:30<00:00,  1.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running bm25 on java's answer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [09:43<00:00,  1.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running tfidf on java's title...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [36:24<00:00,  2.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running tfidf on java's question...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [36:35<00:00,  2.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running tfidf on java's answer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [37:14<00:00,  2.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running bm25 on javascript's title...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [11:07<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running bm25 on javascript's question...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [08:39<00:00,  1.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running bm25 on javascript's answer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [09:02<00:00,  1.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running tfidf on javascript's title...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [33:50<00:00,  2.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running tfidf on javascript's question...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [34:14<00:00,  2.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running tfidf on javascript's answer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [34:20<00:00,  2.06s/it]\n"
          ]
        }
      ],
      "source": [
        "langs = [\"python\", \"java\", \"javascript\"]\n",
        "algos = [\"bm25\", \"tfidf\"]\n",
        "components = [\"title\", \"question\", \"answer\"]\n",
        "\n",
        "for lang, algo, component in itertools.product(langs, algos, components):\n",
        "    print(f\"Running {algo} on {lang}'s {component}...\")\n",
        "    run_retrieval_algorithm(lang, algo, component)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Gxo0kgPc9f"
      },
      "source": [
        "- `pkl/` and `result/` directory :\n",
        "\n",
        "```bash\n",
        "pkl/\n",
        "├── java\n",
        "│   ├── corpus_df_dict.pkl\n",
        "│   ├── corpus_dl_dict.pkl\n",
        "│   └── corpus_tf_dict.pkl\n",
        "├── javascript\n",
        "│   ├── corpus_df_dict.pkl\n",
        "│   ├── corpus_dl_dict.pkl\n",
        "│   └── corpus_tf_dict.pkl\n",
        "└── python\n",
        "    ├── corpus_df_dict.pkl\n",
        "    ├── corpus_dl_dict.pkl\n",
        "    └── corpus_tf_dict.pkl\n",
        "\n",
        "result/\n",
        "├── java_bm25_answer.txt\n",
        "├── java_bm25_question.txt\n",
        "├── java_bm25_title.txt\n",
        "├── javascript_bm25_answer.txt\n",
        "├── javascript_bm25_question.txt\n",
        "├── javascript_bm25_title.txt\n",
        "├── javascript_tfidf_answer.txt\n",
        "├── javascript_tfidf_question.txt\n",
        "├── javascript_tfidf_title.txt\n",
        "├── java_tfidf_answer.txt\n",
        "├── java_tfidf_question.txt\n",
        "├── java_tfidf_title.txt\n",
        "├── python_bm25_answer.txt\n",
        "├── python_bm25_question.txt\n",
        "├── python_bm25_title.txt\n",
        "├── python_tfidf_answer.txt\n",
        "├── python_tfidf_question.txt\n",
        "├── python_tfidf_title.txt\n",
        "├── Q4.txt\n",
        "├── Q5.txt\n",
        "└── Q6.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIzJlUsgQStg",
        "outputId": "9f95d506-96ee-49e5-e4cf-5eda38687c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pkl/\n",
            "├── java\n",
            "│   ├── corpus_df_dict.pkl\n",
            "│   ├── corpus_dl_dict.pkl\n",
            "│   └── corpus_tf_dict.pkl\n",
            "├── javascript\n",
            "│   ├── corpus_df_dict.pkl\n",
            "│   ├── corpus_dl_dict.pkl\n",
            "│   └── corpus_tf_dict.pkl\n",
            "└── python\n",
            "    ├── corpus_df_dict.pkl\n",
            "    ├── corpus_dl_dict.pkl\n",
            "    └── corpus_tf_dict.pkl\n",
            "\n",
            "3 directories, 9 files\n",
            "result/\n",
            "├── java_bm25_answer.txt\n",
            "├── java_bm25_question.txt\n",
            "├── java_bm25_title.txt\n",
            "├── javascript_bm25_answer.txt\n",
            "├── javascript_bm25_question.txt\n",
            "├── javascript_bm25_title.txt\n",
            "├── javascript_tfidf_answer.txt\n",
            "├── javascript_tfidf_question.txt\n",
            "├── javascript_tfidf_title.txt\n",
            "├── java_tfidf_answer.txt\n",
            "├── java_tfidf_question.txt\n",
            "├── java_tfidf_title.txt\n",
            "├── python_bm25_answer.txt\n",
            "├── python_bm25_question.txt\n",
            "├── python_bm25_title.txt\n",
            "├── python_tfidf_answer.txt\n",
            "├── python_tfidf_question.txt\n",
            "├── python_tfidf_title.txt\n",
            "├── Q4.txt\n",
            "├── Q5.txt\n",
            "└── Q6.txt\n",
            "\n",
            "0 directories, 21 files\n"
          ]
        }
      ],
      "source": [
        "! tree pkl/\n",
        "! tree result/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "eCVQlmDSPMM0"
      },
      "outputs": [],
      "source": [
        "ID = \"########\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8r09j33RXIT",
        "outputId": "64c3b4bc-8c4b-445d-f7e0-511d56ff9046"
      },
      "outputs": [],
      "source": [
        "! mkdir result/\n",
        "! cp -r pkl/ result/\n",
        "! cp -r result/ result/\n",
        "! zip -r {ID}.zip result/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
